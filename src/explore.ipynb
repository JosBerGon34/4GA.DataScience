{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- EPLORACION DE DATOS: A)EXTRACCION DE DATOS, B) CATEGORIZACION DE DATOS C)LIMPIEZA DE DATOS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.A) Obtener dataframe de datos a partir del csv.\n",
    "#https://docs.python.org/3/library/csv.html csv related.\n",
    "#rawdata as rd\n",
    "rd= \"https://raw.githubusercontent.com/4GeeksAcademy/logistic-regression-project-tutorial/main/bank-marketing-campaign-data.csv\" \n",
    "#dataframe rawdata as df_rd\n",
    "df_rd = pd.read_csv(rd)\n",
    "df_rd.to_csv('/workspaces/EDA_ML.LogisticRegresion/data/raw/df_rd.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.B) CATEGORIZACION DE DATOS.\n",
    "#INFO y Head para ver tipos de variables. Tambien con la extension \"DataWrangler podemos ver archivos csv y dataframes\"\n",
    "df_rd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.B) El dataframe no tiene el formato deseado , observamos que los identificadores de columna estan separados por puntos y coma,\n",
    "#filtramos el csv raw para darle formato y sobreescribimos nuestro dataframe\n",
    "df_rd = pd.read_csv(rd, sep=\";\")\n",
    "df_rd.to_csv('/workspaces/EDA_ML.LogisticRegresion/data/raw/df_rd.csv')\n",
    "df_rd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.B) ClassDatos\n",
    "#Utilizamos una función para separar variables numericas y categoricas en un dataframe.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#VOY A CAMBIAR EL NOMBRE A LA VARIABLE TARGET POR DEPOSIT\n",
    "df_rd = df_rd.rename(columns={'y': 'deposit'})\n",
    "def vartype(df):\n",
    "\n",
    "\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    \n",
    "    df_tipos = pd.DataFrame({'Tipo': ['Numérica']*len(num_cols) + ['Categórica']*len(cat_cols),\n",
    "                            'Columna': num_cols + cat_cols})\n",
    "    total_num = len(num_cols)\n",
    "    total_cat = len(cat_cols)\n",
    "    total_general = total_num + total_cat\n",
    "    # Crear una lista de diccionarios para las nuevas filas\n",
    "    new_rows = [\n",
    "        {'Tipo': 'Total Numérico', 'Columna': total_num},\n",
    "        {'Tipo': 'Total Categórico', 'Columna': total_cat},\n",
    "        {'Tipo': 'Total General', 'Columna': total_general},\n",
    "    ]\n",
    "\n",
    "    \n",
    "    df_tipos = pd.concat([df_tipos, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    \n",
    "    return df_tipos\n",
    "\n",
    "df_rd_vars= vartype(df_rd)\n",
    "print(df_rd_vars)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.C) Limpieza de datos\n",
    "#Ahora buscamos datos nulos y duplicados.\n",
    "print(df_rd.duplicated(),'\\n',df_rd.isnull().sum())\n",
    "#Observamos que hay datos duplicados pero no nulos. Modificaremos el dataframe sin duplicados.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.C) Limpieza de datos\n",
    "df_rd = df_rd.drop_duplicates().reset_index(drop = True)\n",
    "df_rd.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.C) Limpieza de datos\n",
    "df_rd.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUESTRA VARIABLE TARGET, NOS LA HA DADO NUESTRO CLIENTE, ES LA AFIRMACION DE QUE UN CLIENTE QUIERA CONTRATAR UNA CUENTA DE DEPOSITOS A\n",
    "LARGO PLAZO, VARIABLE ''y'' en el DATAFRAME, la ''y'' del dataframe nos indica, que de los clientes ya existentes de este banco,\n",
    "quienes tienen ya una cuenta de ahorro contratada (deposito a largo plazo).\n",
    "EL OBJETIVO: Entrenar una clasificacion de regresion logistica con los datos ya existentes, poder ahorrar tiempo y asegurarnos\n",
    "en que la busqueda del perfil del cliente sea exacta. Buscaremos relaciones positivas y excluiremos perfiles con relaciones negativas, dentro de nuestro dataframe e intentaremos conseguir un modelo eficiente para esta tarea.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 2) ANÁLISIS UNIVARIABLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1) VARIABLES UNIVARIADAS CATEGORICAS.\n",
    "\n",
    "print(df_rd_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIABLES CATEGÓRICAS.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#graficaremos de 4 en 4\n",
    "#1grupo: job marital education default\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "sns.countplot(ax=axs[0, 0],\n",
    "              data=df_rd,\n",
    "              x=\"job\",\n",
    "              hue=\"job\",\n",
    "              order=df_rd['job'].sort_values().value_counts().index)\n",
    "axs[0, 0].set_xticklabels(axs[0, 0].get_xticklabels(), rotation=90)\n",
    "\n",
    "\n",
    "sns.countplot(ax=axs[0, 1],\n",
    "              data=df_rd,\n",
    "              x=\"marital\",\n",
    "              hue=\"marital\",\n",
    "              order=df_rd['marital'].sort_values().value_counts().index)\n",
    "axs[0, 1].set_xticklabels(axs[0, 1].get_xticklabels(), rotation=90)\n",
    "\n",
    "\n",
    "sns.countplot(ax=axs[1, 0],\n",
    "              data=df_rd,\n",
    "              x=\"education\",\n",
    "              hue=\"education\",\n",
    "              order=df_rd['education'].sort_values().value_counts().index)\n",
    "axs[1, 0].set_xticklabels(axs[1, 0].get_xticklabels(), rotation=90)\n",
    "\n",
    "\n",
    "sns.countplot(ax=axs[1, 1],\n",
    "              data=df_rd,\n",
    "              x=\"default\",\n",
    "              hue=\"default\",\n",
    "              order=df_rd['default'].sort_values().value_counts().index)\n",
    "axs[1, 1].set_xticklabels(axs[1, 1].get_xticklabels(), rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#grupo: housing loan contact month\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "sns.countplot(ax=axs[0, 0],\n",
    "              data=df_rd,\n",
    "              x=\"housing\",\n",
    "              hue=\"housing\",\n",
    "              order=df_rd['housing'].sort_values().value_counts().index)\n",
    "axs[0, 0].set_xticklabels(axs[0, 0].get_xticklabels(), rotation=90)\n",
    "\n",
    "\n",
    "sns.countplot(ax=axs[0, 1],\n",
    "              data=df_rd,\n",
    "              x=\"loan\",\n",
    "              hue=\"loan\",\n",
    "              order=df_rd['loan'].sort_values().value_counts().index)\n",
    "axs[0, 1].set_xticklabels(axs[0, 1].get_xticklabels(), rotation=90)\n",
    "\n",
    "\n",
    "sns.countplot(ax=axs[1, 0],\n",
    "              data=df_rd,\n",
    "              x=\"contact\",\n",
    "              hue=\"contact\",\n",
    "              order=df_rd['contact'].sort_values().value_counts().index)\n",
    "axs[1, 0].set_xticklabels(axs[1, 0].get_xticklabels(), rotation=90)\n",
    "\n",
    "\n",
    "sns.countplot(ax=axs[1, 1],\n",
    "              data=df_rd,\n",
    "              x=\"month\",\n",
    "              hue=\"month\",\n",
    "              order=df_rd['month'].sort_values().value_counts().index)\n",
    "axs[1, 1].set_xticklabels(axs[1, 1].get_xticklabels(), rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#3grupo poutcome day_of_week  deposit\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "sns.countplot(ax=axs[0, 0],\n",
    "              data=df_rd,\n",
    "              x=\"poutcome\",\n",
    "              hue=\"poutcome\",\n",
    "              order=df_rd['poutcome'].sort_values().value_counts().index)\n",
    "axs[0, 0].set_xticklabels(axs[0, 0].get_xticklabels(), rotation=90)\n",
    "\n",
    "\n",
    "sns.countplot(ax=axs[0, 1],\n",
    "              data=df_rd,\n",
    "              x=\"day_of_week\",\n",
    "              hue=\"day_of_week\",\n",
    "              order=df_rd['day_of_week'].sort_values().value_counts().index)\n",
    "axs[0, 1].set_xticklabels(axs[0, 1].get_xticklabels(), rotation=90)\n",
    "\n",
    "\n",
    "sns.countplot(ax=axs[1, 0],\n",
    "              data=df_rd,\n",
    "              x=\"deposit\",\n",
    "              hue=\"deposit\",\n",
    "              order=df_rd['deposit'].sort_values().value_counts().index)\n",
    "axs[1, 0].set_xticklabels(axs[1, 0].get_xticklabels(), rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_rd_vars[df_rd_vars['Tipo'] == 'Categórica']['Columna'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRIMERO DE TODO TRADUCIMOS LAS VARIABLES CATEGORICAS AL ESPAÑOL:\n",
    "    'job': 'trabajo',\n",
    "    'marital': 'estado_civil',\n",
    "    'education': 'educacion',\n",
    "    'default': 'impago',\n",
    "    'housing': 'vivienda',\n",
    "    'loan': 'prestamo',\n",
    "    'contact': 'contacto',\n",
    "    'month': 'mes',\n",
    "    'day_of_week': 'dia_semana',\n",
    "    'poutcome': 'resultado_anterior',\n",
    "    'y': 'variable_objetivo'  (true o false) personas con cuenta de ahorros/depositos a largo plazo.\n",
    "CONCLUSIONES: Estos son los datos de los clientes dados de alta, la variable month  es mas relevante que  day_of_week la cual su distribucion se mantiene homogenea durante todos los dias de la semana.\n",
    "puedo observar una distribucion pareja en muchas de las variables, incluido patrones similares con la variable target. A priori mantendre todas la variables,\n",
    "hasta observar la correlacion multivariable, pero estoy convencido de que mas de 1 variable dan informacion redundante, por lo que en nuestra clasificacion,\n",
    "podremos obviarlas para el calculo en los modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2) VARIABLES UNIVARIADAS NUMERICAS.\n",
    "print(df_rd_vars[df_rd_vars['Tipo'] == 'Numérica']['Columna'].unique())\n",
    "#Preparo los datos para graficar:\n",
    "maxage= df_rd['age'].max()\n",
    "maxdur= df_rd['duration'].max()\n",
    "maxcam= df_rd['campaign'].max()\n",
    "maxpday= df_rd['pdays'].max()\n",
    "maxprev= df_rd['previous'].max()\n",
    "maxconspr= df_rd['cons.price.idx'].max()\n",
    "maxcons= df_rd['cons.conf.idx'].max()\n",
    "maxeur= df_rd['euribor3m'].max()\n",
    "maxemp= df_rd['nr.employed'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2) VARIABLES UNIVARIADAS NUMERICAS. Gráficos.\n",
    "#1grupo: 'age' 'duration' \n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "\n",
    "\n",
    "sns.histplot(ax= axs[0,0], data=df_rd, x='age', color=\"red\").set_xlim(0, maxage)\n",
    "axs[0, 0].set_xticklabels(axs[0, 0].get_xticklabels(), rotation=90)\n",
    "\n",
    "sns.boxplot(ax=axs[1,0], data=df_rd, x='age',color=\"red\")\n",
    "axs[0, 0].set_xticklabels(axs[0, 0].get_xticklabels(), rotation=90)\n",
    "\n",
    "########\n",
    "\n",
    "sns.histplot(ax= axs[0,1], data=df_rd, x='duration',color=\"green\").set_xlim(0, maxdur)\n",
    "axs[0, 1].set_xticklabels(axs[0, 1].get_xticklabels(), rotation=90)\n",
    "\n",
    "sns.boxplot(ax=axs[1,1], data=df_rd, x='duration',color=\"green\")\n",
    "axs[1, 1].set_xticklabels(axs[1, 1].get_xticklabels(), rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#2grupo 'campaign' 'pdays'\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "\n",
    "\n",
    "sns.histplot(ax= axs[0,0], data=df_rd, x='campaign', color=\"red\").set_xlim(0, maxcam)\n",
    "axs[0, 0].set_xticklabels(axs[0, 0].get_xticklabels(), rotation=90)\n",
    "\n",
    "sns.boxplot(ax=axs[1,0], data=df_rd, x='campaign',color=\"red\")\n",
    "axs[0, 0].set_xticklabels(axs[0, 0].get_xticklabels(), rotation=90)\n",
    "\n",
    "########\n",
    "\n",
    "sns.histplot(ax= axs[0,1], data=df_rd, x='pdays',color=\"green\").set_xlim(0, maxpday)\n",
    "axs[0, 1].set_xticklabels(axs[0, 1].get_xticklabels(), rotation=90)\n",
    "\n",
    "sns.boxplot(ax=axs[1,1], data=df_rd, x='pdays',color=\"green\")\n",
    "axs[1, 1].set_xticklabels(axs[1, 1].get_xticklabels(), rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#3grupo 'previous' 'emp.var.rate'\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "\n",
    "\n",
    "sns.histplot(ax= axs[0,0], data=df_rd, x='previous', color=\"red\").set_xlim(0, maxprev)\n",
    "axs[0, 0].set_xticklabels(axs[0, 0].get_xticklabels(), rotation=90)\n",
    "\n",
    "sns.boxplot(ax=axs[1,0], data=df_rd, x='previous',color=\"red\")\n",
    "axs[0, 0].set_xticklabels(axs[0, 0].get_xticklabels(), rotation=90)\n",
    "\n",
    "########\n",
    "\n",
    "sns.histplot(ax= axs[0,1], data=df_rd, x='emp.var.rate',color=\"green\").set_xlim(0, maxemp)\n",
    "axs[0, 1].set_xticklabels(axs[0, 1].get_xticklabels(), rotation=90)\n",
    "\n",
    "sns.boxplot(ax=axs[1,1], data=df_rd, x='emp.var.rate',color=\"green\")\n",
    "axs[1, 1].set_xticklabels(axs[1, 1].get_xticklabels(), rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#4grupo 'cons.price.idx' 'cons.conf.idx'\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "\n",
    "\n",
    "sns.histplot(ax= axs[0,0], data=df_rd, x='cons.price.idx', color=\"red\").set_xlim(0, maxconspr)\n",
    "axs[0, 0].set_xticklabels(axs[0, 0].get_xticklabels(), rotation=90)\n",
    "\n",
    "sns.boxplot(ax=axs[1,0], data=df_rd, x='cons.price.idx',color=\"red\")\n",
    "axs[0, 0].set_xticklabels(axs[0, 0].get_xticklabels(), rotation=90)\n",
    "\n",
    "########\n",
    "\n",
    "sns.histplot(ax= axs[0,1], data=df_rd, x='cons.conf.idx',color=\"green\").set_xlim(0, maxconspr)\n",
    "axs[0, 1].set_xticklabels(axs[0, 1].get_xticklabels(), rotation=90)\n",
    "\n",
    "sns.boxplot(ax=axs[1,1], data=df_rd, x='cons.conf.idx',color=\"green\")\n",
    "axs[1, 1].set_xticklabels(axs[1, 1].get_xticklabels(), rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2)CONCLUSIONES ANÁLISIS UNIVARIABLE. VARIABLES NUMÉRICAS:\n",
    "En muchas de las variables existen outliers que distorsionan la distribucion de los datos, aun asi puedo observar que hay conjuntos de variables que trazan una distribucion similar entre ellas,\n",
    "puediendo destacar distrbuciones asimetricas en ambos lados del rango de valor de Y, ademas de un par de variables distrbuidas simetricamente.\n",
    "Veremos como afecta a la linealidad intervariable, tras la eliminacion de outliers, en el caso que la cantidad de muestras fuera de los rangos cuartilicos sea infima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 3: ANALISIS MULTIVARIABLES: \n",
    "3.1) ANÁLISIS Categóricas-Categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_rd_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PASO 3.1) ANÁLISIS MULTIVARIALE: NUM-CAT\n",
    "#Antes de realizar un análisis exhaustivo entre variables voy a mencionar que el estudio de variables numericas-numericas, categoricas-categoricas, y numericas-categóricas\n",
    "# Se puede englobar en un solo paso gracias a la factorización de variables categoricas por un numero entero,\n",
    "# Este numero es una lista de enteros, donde se le asigna un valor a cada valor unico de la columna por orden de aparicion en los rows del dataframe.\n",
    "# Solo realizare exclusivamente la observacion en las variables categoricas categoricas.\n",
    "#Tras esto realizare la factorizacion y hare un análisis Numerico Numerico global.\n",
    "# en el eje X subdividiremos las muestras en funcion de nuestra variable target 'deposit' categorica creando dos subgrupos de barras.\n",
    "\n",
    "print(df_rd_vars[df_rd_vars['Tipo'] == 'Categórica']['Columna'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1) SE GUARDARAN TODAS LAS VARIABLES FACTORIZADAS EN UN DICCIONARIO, ADEMAS SE LES CAMBIARA EL NOMBRE A LAS VARIABLES PARA IDENTIFICAR CUAL HA SIDO FACTORIZADA con la terminacion '_fc'.\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def JsonFCZRename(df, categorical_cols, output_dir, suffix=\"_fc\"):\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        mapping_dict = {value: str(index) for value, index in zip(le.classes_, le.transform(le.classes_))}\n",
    "        print(f\"Column: {col}\")\n",
    "        print(mapping_dict)\n",
    "        filename = f\"{col}_factors.json\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(mapping_dict, f, indent=4)\n",
    "        df.rename(columns={col: col + suffix}, inplace=True)\n",
    "\n",
    "    return df\n",
    "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome', 'deposit']\n",
    "JsonFCZRename(df_rd, categorical_cols, '/workspaces/EDA_ML.LogisticRegresion/data/processed/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vartype(df):\n",
    "\n",
    "\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    \n",
    "    df_tipos = pd.DataFrame({'Tipo': ['Numérica']*len(num_cols) + ['Categórica']*len(cat_cols),\n",
    "                            'Columna': num_cols + cat_cols})\n",
    "    total_num = len(num_cols)\n",
    "    total_cat = len(cat_cols)\n",
    "    total_general = total_num + total_cat\n",
    "    # Crear una lista de diccionarios para las nuevas filas\n",
    "    new_rows = [\n",
    "        {'Tipo': 'Total Numérico', 'Columna': total_num},\n",
    "        {'Tipo': 'Total Categórico', 'Columna': total_cat},\n",
    "        {'Tipo': 'Total General', 'Columna': total_general},\n",
    "    ]\n",
    "\n",
    "    \n",
    "    df_tipos = pd.concat([df_tipos, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    \n",
    "    return df_tipos\n",
    "df_rd_var_fc=vartype(df_rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2)Analisis multivariable Global (NUM-NUM_FC(Categoricas factorizadas))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.pairplot(data=df_rd, height=1, aspect=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.3) MAPA DE CALOR GLOBAL, MATRIZ DE CONFUSION.\n",
    "#Primero de todo vamos a centrar nuestra var target en la diagonal de la matriz.\n",
    "target_var = \"deposit_fc\"  \n",
    "df_heatmap = df_rd.reindex(columns=[target_var] + list(df_rd.columns.difference([target_var])))\n",
    "#Ahora representamos el grafico.\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(df_heatmap.corr(), annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones del mapa de calor:\n",
    "Nuestra variable target, desgraciadamente tiene correlaciones tanto positivas como negativas.\n",
    "En el sentido positivo tiene una gran correlacion con \"duration\"(num 0.41), con previous(num 0.23), y con poutcome(cat fcz 0.13)\n",
    "Como me han pedido solo hacer una clasificacion y hacer un modelo que funcione no necesito saber que es cada cosa y el porque.\n",
    "Hay varias variables que tambien en algun punto de las muestras hacen que la opcion de tener un deposito no sea algo bueno, tienen una relacion negativa:\n",
    "nr.employed(num -0.35, pdays -0.32, euribor3m -0.31, emp.var.rate -0.30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 4) FEATURE ENGINEERING.\n",
    "4.1) VARIABLES A FUSIONAR/ELIMINAR.\n",
    "\n",
    "A)'nr.employed+emp.var.rate'(0.91),'nr.employed+euribor3m'(0.95),'euribor3m+emp.var.rate'(0.97).\n",
    " HAY 4 VARIABLES QUE ENTRE ELLAS TIENE UNA CORRELACION CERCANA A 1, no voy a fusionarlas porque tienen una escala muy diferente.\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_rd_var_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats1=df_rd['nr.employed'].describe()\n",
    "stats2=df_rd['euribor3m'].describe()\n",
    "stats3=df_rd['emp.var.rate'].describe()\n",
    "print(stats1,'\\n',stats2,'\\n',stats3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PASO 4.2) ANÁLISIS DE LOS VALORES ATÍPICOS(OUTLIERS)\n",
    "df_rd.describe()\n",
    "#Para hacernos una idea de donde hay valores atípicos empezaremos por observar el describe de nuestras variables.\n",
    "#Tras este paso empezaremos a graficar los graficos de caja que nos ayudaran a visualizar los outlier para cada variable relevante.\n",
    "fig, ax = plt.subplots(10, 2, figsize=(18, 15))\n",
    "sns.boxplot(ax = ax[0,0], data = df_rd, x='age')\n",
    "sns.boxplot(ax = ax[0,1], data = df_rd, x='job_fc')\n",
    "sns.boxplot(ax = ax[1,0], data = df_rd, x='marital_fc')\n",
    "sns.boxplot(ax = ax[1,1], data = df_rd, x='default_fc')\n",
    "sns.boxplot(ax = ax[2,0], data = df_rd, x='housing_fc')\n",
    "sns.boxplot(ax = ax[2,1], data = df_rd, x='loan_fc')\n",
    "sns.boxplot(ax = ax[3,0], data = df_rd, x='contact_fc')\n",
    "sns.boxplot(ax = ax[3,1], data = df_rd, x='month_fc')\n",
    "sns.boxplot(ax = ax[4,0], data = df_rd, x='day_of_week_fc')\n",
    "sns.boxplot(ax = ax[4,1], data = df_rd, x='duration')\n",
    "sns.boxplot(ax = ax[5,0], data = df_rd, x='campaign')\n",
    "sns.boxplot(ax = ax[5,1], data = df_rd, x='pdays')\n",
    "sns.boxplot(ax = ax[6,0], data = df_rd, x='previous')\n",
    "sns.boxplot(ax = ax[6,1], data = df_rd, x='poutcome_fc')\n",
    "sns.boxplot(ax = ax[7,0], data = df_rd, x='emp.var.rate')\n",
    "sns.boxplot(ax = ax[7,1], data = df_rd, x='cons.price.idx')\n",
    "sns.boxplot(ax = ax[8,0], data = df_rd, x='cons.conf.idx')\n",
    "sns.boxplot(ax = ax[8,1], data = df_rd, x='euribor3m')\n",
    "sns.boxplot(ax = ax[9,0], data = df_rd, x='nr.employed')\n",
    "sns.boxplot(ax = ax[9,1], data = df_rd, x='deposit_fc')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2)\n",
    "#dataframe sin outliers.\n",
    "df_rd_sol= df_rd.copy()\n",
    "df_rd_sol.to_csv('/workspaces/EDA_ML.LogisticRegresion/data/processed/dataframes/df_rd_sol.csv')\n",
    "#dataframe con outliers.\n",
    "df_rd_col= df_rd.copy()\n",
    "df_rd_col.to_csv('/workspaces/EDA_ML.LogisticRegresion/data/processed/dataframes/df_rd_col.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "#4.3) Eliminamos outliers para df_rd_sol \\\\sin outliers\\\\\n",
    "def LimsOlSaveJson(df, columns, output_path):\n",
    "    for column in columns:\n",
    "        stats = df[column].describe()\n",
    "        iqr = stats['75%'] - stats['25%']\n",
    "        ul = stats['75%'] + (2 * iqr)\n",
    "        ll = max(stats['25%'] - (2 * iqr), 0)\n",
    "        limits = {\"upper_limit\": ul, \"lower_limit\": ll}\n",
    "        file_path = f\"{output_path}/limits_{column}.json\"\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(limits, f, indent=4)\n",
    "        df[column] = df[column].apply(lambda x: ul if x > ul else ll if x < ll else x)\n",
    "    return df\n",
    "#hacemos una lista de todas las variables, como hemos factorizado todas las variables categoricas, todas deberian ser numericas.\n",
    "num_vars= df_rd_sol.select_dtypes(include=np.number).columns.tolist()\n",
    "LimsOlSaveJson(df_rd_sol, num_vars,'/workspaces/EDA_ML.LogisticRegresion/data/processed/OutliersLims')\n",
    "df_rd_sol.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) FEATURE ENGINEERING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.1) FUNCION SPLIT TRAIN/TEST\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "def split_and_save_data(df, target_column, output_dir, test_size=0.25, random_state=42, filename_prefix=\"\"):\n",
    "  X = df.drop(columns=target_column)\n",
    "  y = df[target_column]\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "  train_dir = os.path.join(output_dir, \"train\")\n",
    "  test_dir = os.path.join(output_dir, \"test\")\n",
    "  os.makedirs(train_dir, exist_ok=True)\n",
    "  os.makedirs(test_dir, exist_ok=True)\n",
    "  X_train.to_csv(os.path.join(train_dir, f\"{filename_prefix}_X_train.csv\"), index=False)\n",
    "  X_test.to_csv(os.path.join(test_dir, f\"{filename_prefix}_X_test.csv\"), index=False)\n",
    "  y_train.to_csv(os.path.join(train_dir, f\"{filename_prefix}_y_train.csv\"), index=False)\n",
    "  y_test.to_csv(os.path.join(test_dir, f\"{filename_prefix}_y_test.csv\"), index=False)\n",
    "output_dir = '/workspaces/EDA_ML.LogisticRegresion/data/processed/dataframes'\n",
    "target_column = 'deposit_fc'\n",
    "split_and_save_data(df_rd_sol, target_column, output_dir, filename_prefix=\"sol\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2) Escalaremos las variables. MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.2) MinMaxScale\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "def minmax(df_train, df_test, prefix, output_train, output_test):\n",
    "  X_train = pd.read_csv(df_train)\n",
    "  X_test = pd.read_csv(df_test)\n",
    "  df_name_train = os.path.splitext(os.path.basename(df_train))[0]\n",
    "  df_name_test = os.path.splitext(os.path.basename(df_test))[0]\n",
    "  scaler = MinMaxScaler()\n",
    "  scaler.fit(X_train)\n",
    "  X_train_minmax = scaler.transform(X_train)\n",
    "  X_test_minmax = scaler.transform(X_test)\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "  X_train_minmax = pd.DataFrame(X_train_minmax, index=X_train.index, columns=X_train.columns)\n",
    "  X_train_minmax.to_csv(os.path.join(output_train, f\"{df_name_train}_minmax.csv\"), index=False)\n",
    "  X_test_minmax = pd.DataFrame(X_test_minmax, index=X_test.index, columns=X_test.columns)\n",
    "  X_test_minmax.to_csv(os.path.join(output_test, f\"{df_name_test}_minmax.csv\"), index=False)\n",
    "  joblib.dump(scaler, os.path.join(output_test, f\"minmax_{prefix}.sav\"))\n",
    "  return X_train_minmax, X_test_minmax\n",
    "#LLamada a funcion e inputs.\n",
    "outputtrain = '/workspaces/EDA_ML.LogisticRegresion/data/processed/dataframes/train/'\n",
    "outputtest = '/workspaces/EDA_ML.LogisticRegresion/data/processed/dataframes/test/'\n",
    "#Prefix = Identificador que hace referencia al tipo de DF, con outliers o sin , col o sol.\n",
    "X_train_scaled, X_test_scaled = minmax('/workspaces/EDA_ML.LogisticRegresion/data/processed/dataframes/train/sol_X_train.csv', \n",
    "                                       '/workspaces/EDA_ML.LogisticRegresion/data/processed/dataframes/test/sol_X_test.csv', \n",
    "                                       prefix='sol', output_train=outputtrain,output_test=outputtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3) FEATURE SELECTION: SELECTKBEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rd_var_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job_fc</th>\n",
       "      <th>marital_fc</th>\n",
       "      <th>education_fc</th>\n",
       "      <th>contact_fc</th>\n",
       "      <th>day_of_week_fc</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.341301</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.698753</td>\n",
       "      <td>0.956926</td>\n",
       "      <td>0.859735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.087649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269680</td>\n",
       "      <td>0.146679</td>\n",
       "      <td>0.512287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.276228</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.698753</td>\n",
       "      <td>0.956926</td>\n",
       "      <td>0.859735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.669135</td>\n",
       "      <td>0.981183</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.298805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.669135</td>\n",
       "      <td>0.981183</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    job_fc  marital_fc  education_fc  contact_fc  day_of_week_fc  \\\n",
       "0  0.433333  0.363636    0.333333      0.000000         1.0            0.00   \n",
       "1  0.116667  0.727273    0.666667      0.428571         0.0            1.00   \n",
       "2  0.533333  0.000000    0.333333      0.857143         1.0            0.00   \n",
       "3  0.383333  0.545455    0.333333      1.000000         1.0            0.25   \n",
       "4  0.400000  0.090909    0.666667      0.142857         0.0            0.50   \n",
       "\n",
       "   duration  campaign  emp.var.rate  cons.price.idx  euribor3m  nr.employed  \n",
       "0  0.341301  0.333333      0.785714        0.698753   0.956926     0.859735  \n",
       "1  0.087649  0.000000      0.000000        0.269680   0.146679     0.512287  \n",
       "2  0.276228  0.333333      0.785714        0.698753   0.956926     0.859735  \n",
       "3  1.000000  0.000000      1.000000        0.669135   0.981183     1.000000  \n",
       "4  0.298805  0.000000      1.000000        0.669135   0.981183     1.000000  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "#de 20 numericas bajamos con k a 12\n",
    "selection_model = SelectKBest(chi2, k = 12)\n",
    "X_train = pd.read_csv('/workspaces/EDA_ML.LogisticRegresion/data/processed/dataframes/train/sol_X_train_minmax.csv')\n",
    "y_train = pd.read_csv('/workspaces/EDA_ML.LogisticRegresion/data/processed/dataframes/train/col_y_train.csv')\n",
    "X_test = pd.read_csv('/workspaces/EDA_ML.LogisticRegresion/data/processed/dataframes/test/sol_X_test_minmax.csv')\n",
    "selection_model.fit(X_train, y_train)\n",
    "ix = selection_model.get_support()\n",
    "X_train_sel = pd.DataFrame(selection_model.transform(X_train), columns = X_train.columns.values[ix])\n",
    "X_test_sel = pd.DataFrame(selection_model.transform(X_test), columns = X_test.columns.values[ix])\n",
    "X_train_sel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
